# Тестовое задание для python-разработчика

### Задача

Реализовать парсер произвольных сайтов и HTTP API для просмотра результатов парсинга. 

#### Требования к парсеру

- должен принимать на вход адрес сайта(address), максимальную глубину обхода(depth) и количество одновременно загружаемых страниц(amount)
- на глубине 0, нужно сохранить только данные с переданной в параметрах страницы
- на глубине n+1, нужно сохранить данные страниц, на которые есть ссылки на странице глубины n

- под данными страницы понимается её `html`, `url` и `title`

  Пусть будет таблица в базе url, title, html.

    url - varchar(??)         - считаем уникальным.
    title - varchar(??)       - тут все просто.
    html_path - varchar(??)   - тут будет путь до файла.
    status - varchar(??)      - тут храним текстом доп информацию (например, если не загрузили, то кладем ексепшен.)

    db_name - parser
    password - parser

    Сохранение будет происходить sql командой в базу:
    ```sql

      insert into parser (url, title, html_path, now(), status)
      values ('example.com', 'first page', '/tmp/html/example.com', '2022-11-25' 'ok')
    ```

- каждая страница должна сохраняться не больше одного раза
  Для простоты предположим, что по url страницы мы можем однозначно отличить одну страницу от другой.
  Тогда можно в редис складывать все уникальные url-ы.
  Когда краулер будет собирать урлы, перед созданием задачи, проверит что такого урла в редисе. 
  Структура пусть будет неупорядоченное множество (уникальность из коробки). (SADD - добавить.)


  Нужно сделать механизм PubSub.
  - краулер будет собирать ссылки и кидать их в очередь (Publisher)
  - парсер html будет подписан на канал, при получении сообщения (задачи на парсинг), будет заходить на адрес и сохранять тело страницы.
  - имя пусть будет в формате `title`. Имя ограничим 16 символами, не более (пока не знаю зачем.)
  - путь до страницы что-то типа /tmp/html/example.com - все будем складывать в один каталог (хотя можно настроить иерархическое хранилище.)

пример вызова скрипта > python3 -m parse.py -u url -d 0-n -a 0-m
                        python3 -m parse.py -u habr.com -d 0 -a 3

                        Пусть глубина depth по умолчанию будет равна 5
                        Пусть количество страниц amount по умолчанию будет 50

- Если depth или amount указаны с ошибкой (не число, или еще что-то, добавить проверку и эксепшены.)
  
  ```python

      class ParserArgumentError(Exception):
    """
    Аргумент указан не верно

    Пример: python3 -m parse.py -u habr.com -d 0 -a 3 
            -u url сайта
            -d ...
            -a ...
    """
    
  ```
  


#### Требования к HTTP API

Авторизации не будет, все запросы GET.

- нужны два метода

  - получение списка страниц с их адресами и заголовками (с возможностью поиска по обоим полям)
    https://foo.bar/pages

    Запрос к базе:

    ```sql

      SELECT id, address, title from db
      where address = 'someaddres' and title = 'sometitle';

    ```

    https://foo.bar/pages/?addres='someaddress' - 
    https://foo.bar/pages/?title=sometitle'

  - получение html конкртеной страницы
    https://foo.bar/pages/1
    ```sql
      SELECT * from db 
      where id = 1';

      -- можно запрашивать по урлу, и проиндексировать этот столбец.
      
    ```
- спецификация в машиночитаемом формате (Open api, etc.)


#### Общие требования к решению

- язык реализации `python3.7+`
- весь ввод-вывод асинхронный (asyncio или trio)
- запуск в docker
- оформление в виде git-репозитория 
- требований кроме описанных выше нет, что в частности означает, что можно использовать любые БД/фреймворк/библиотеки



#### Будет большим плюсом

- наличие аннотаций типов с автоматизированной проверкой mypy
- наличие тестов